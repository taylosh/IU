# -*- coding: utf-8 -*-
"""Assignment02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jgwNdz58zCrymqZy4QYQQvbvQKHgH0Or

# L635-Fall2025-Assignment 02: Fine-tuning Speech Foundation Models using ESPnet EZ


Main references:
- [ESPnet repository](https://github.com/espnet/espnet)
- [ESPnet documentation](https://espnet.github.io/espnet/)
- [ESPnet-EZ repo](https://github.com/espnet/espnet/tree/master/espnetez)

## Important Notes
- Please submit PDF files of your completed notebooks to Canvas. You can print the notebook using `File -> Print` in the menu bar.

## Acknowledgement
- This homework is adapted from the ESPnet online demos and tutorials.

## Install ESPnet
- The temporary version we used for Assignment 1 just got merged to ESPNET. We are now installing espnet as should be. You may see some dependency errors. It should be safe for you to ignore them for now.
"""

!git clone https://github.com/espnet/espnet.git
!cd espnet && pip install .

!pip install espnet-model-zoo # for downloading pre-trained models
!apt install ffmpeg # for audio file processing
!pip install ipywebrtc notebook # for real-time recording
!pip install datasets==3.6.0 # for downloading ASR datasets

"""### Import the dependencies and check the state of installation"""

import torch
import datasets
import espnetez as ez # ESPnet wrapper that simplifies integration. If you get an error when executing this cell, click Runtime -> Restart Session, and rerun from the beginning
import numpy as np
import librosa
from espnet2.bin.s2t_inference import Speech2Text # Core ESPnet module for pre-trained models

print("Installation success!")

"""## Data Processing
For this tutorial, we will use the [FLEURS](https://arxiv.org/abs/2205.12446) dataset from HuggingFace: https://huggingface.co/datasets/google/fleurs .

FLEURS is a 102-language multilingual speech dataset, supporting tasks such as Automatic Speech Recognition (ASR), Speech Translation (ST), and Language Identification (LID).

While the total size of FLEURS is relatively large at ~1000 hours of training data, each individual language only has 7-10 hours of audio.

For this tutorial, we will focus on monolingual ASR for one of the 102 languages.

### Data Downloading
We will first download the data for one language of FLEURS. FLEURS organizes the languages by its ISO2 language code and locale. For example, American English is `en_us`.

**We will use English for the first fine-tuning experiment.** You will have the opportunity to try a different language later on in the assignment.

If you want to download the data for another language, you can map the language name to the ISO2 code using Table 9 in the FLEURS paper: https://arxiv.org/pdf/2205.12446. Then, you can use that to identify the language+region combination using the HuggingFace data previewer: https://huggingface.co/datasets/google/fleurs .

(Please select y for the prompt of running custom code to download the data)
"""

# Commented out IPython magic to ensure Python compatibility.
!mkdir downloads
# %cd downloads
!pip install --upgrade --no-cache-dir gdown
!gdown 1pzBIeUL1H0z-lLaBFyyKQFBbJhDJXRnM
!tar -xzf TIDIGITS_children_boy.tar.gz && ls TIDIGITS_children_boy

"""### Inspect the data"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/
!mkdir TIDIGITS_subset2
!mkdir TIDIGITS_subset2/train
!mkdir TIDIGITS_subset2/validation
!mkdir TIDIGITS_subset2/test

!gdown 17TlHt41TFZPYNQJ3TuMdw8TyW5gufvtC
!python data_prep.py /content/downloads/TIDIGITS_children_boy
!sh move_files.sh

# Datasets library
from datasets import load_dataset, Audio
train_dataset = load_dataset("audiofolder", data_dir=f"/content/TIDIGITS_subset2", split="train")
valid_dataset = load_dataset("audiofolder", data_dir=f"/content/TIDIGITS_subset2", split="validation")
test_dataset = load_dataset("audiofolder", data_dir=f"/content/TIDIGITS_subset2", split="test")

train_dataset = train_dataset.cast_column("audio", Audio(sampling_rate=16000))
valid_dataset = valid_dataset.cast_column("audio", Audio(sampling_rate=16000))
test_dataset = test_dataset.cast_column("audio", Audio(sampling_rate=16000))

"""## Pretrained Model
In low-resource settings, training a model from scratch is unlikely to lead to good results. So instead, we will fine-tune a pre-trained foundation model.

We will use the base version of [OWSM 3.1](https://arxiv.org/pdf/2401.16658), an open-source speech foundation model trained on 180K hours of multilingual ASR and ST.

### Downloading
Since it needs to support many language varieties, OWSM uses ISO3 for the language IDs. The ISO3 code for your language of choice can also be found in Table 9 in the FLEURS paper: https://arxiv.org/pdf/2205.12446
"""

FINETUNE_MODEL="espnet/owsm_v3.1_ebf_base"
owsm_language="eng" # language code in ISO3

pretrained_model = Speech2Text.from_pretrained(
    FINETUNE_MODEL,
    lang_sym=f"<{owsm_language}>",
    beam_size=1,
    device='cuda'
)
torch.save(pretrained_model.s2t_model.state_dict(), 'original.pth')
pretrain_config = vars(pretrained_model.s2t_train_args)
tokenizer = pretrained_model.tokenizer
converter = pretrained_model.converter

"""## Setup Training
We first need to convert the HuggingFace data into a format that ESPnet can read. This can be easily done by defining a `data_info` dictionary that maps each field required for OWSM fine-tuning to a column in our dataset.
"""

'''
pretrained_model -> the pre-trained model we downloaded earlier
tokenizer -> Tokenizes raw text into subwords
converter -> Converts subwords into integer IDs for model input
'''

def tokenize(text):
    return np.array(converter.tokens2ids(tokenizer.text2tokens(text)))
data_info = {
    "speech": lambda d: d['audio']['array'].astype(np.float32), # 1-D raw waveform
    "text": lambda d: tokenize(f"<{owsm_language}><asr><notimestamps> {d['transcription']}"), # tokenized text mapped to integer ids
    "text_prev": lambda d: tokenize("<na>"), # tokenized text of previous utterance for prompting, unused here
    "text_ctc": lambda d: tokenize(d['transcription']), # tokenized text mapped to integer ids for CTC loss, can be different from "text" depending on task
}
test_data_info = {
    "speech": lambda d: d['audio']['array'].astype(np.float32),
    "text": lambda d: tokenize(f"<{owsm_language}><asr><notimestamps> {d['transcription']}"),
    "text_prev": lambda d: tokenize("<na>"),
    "text_ctc": lambda d: tokenize(d['transcription']),
    "text_raw": lambda d: d['transcription'], # raw untokenized text as the reference
}
train_dataset = ez.dataset.ESPnetEZDataset(train_dataset, data_info=data_info)
valid_dataset = ez.dataset.ESPnetEZDataset(valid_dataset, data_info=data_info)
test_dataset = ez.dataset.ESPnetEZDataset(test_dataset, data_info=test_data_info)

"""Next we need to define a function that will pass our pre-trained model to ESPnet. This function here doesn't do much since our setup is simple, but its required for more complex settings."""

# define model loading function
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def build_model_fn(args):
  model = pretrained_model.s2t_model
  model.train()
  print(f'Trainable parameters: {count_parameters(model)}')
  return model

"""### Training
Training requires tuning many hyper-parameters. Here is an initial config to help start you off.
"""

!gdown 1RuOXmN9nyhLSbRZVGHcxlQjOQ-dygtUu
!mkdir config
!mv finetune.yaml config/finetune.yaml

"""Before we begin training, we need to define where our model files and logs will be saved. We also need to override some of the settings used to pre-train the foundation model with our own settings."""

EXP_DIR = f"./exp/finetune"
STATS_DIR = f"./exp/stats_finetune"
finetune_config = ez.config.update_finetune_config(
	's2t',
	pretrain_config,
	f"./config/finetune.yaml"
)

# You can edit your config by changing the finetune.yaml file directly (but make sure you rerun this cell again!)
# You can also change it programatically like this
finetune_config['max_epoch'] = 1
finetune_config['num_iters_per_epoch'] = 500

"""Finally, we just need to pass our model, data, and configs to a trainer."""

trainer = ez.Trainer(
    task='s2t',
    train_config=finetune_config,
    train_dataset=train_dataset,
    valid_dataset=valid_dataset,
    build_model_fn=build_model_fn, # provide the pre-trained model
    data_info=data_info,
    output_dir=EXP_DIR,
    stats_dir=STATS_DIR,
    ngpu=1
)

trainer.collect_stats() # collect audio/text length information to construct batches

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Launch tensorboard before training
# %tensorboard --logdir /content/exp

trainer.train() # every 100 steps takes ~1 min

"""## Inference

Here is a demo to perform inference using the original and fine-tuned model.
"""

id, sample_test_utterance = test_dataset.__getitem__(0)

pretrained_model.s2t_model.cuda()
pretrained_model.device = 'cuda'

d = torch.load("original.pth")
pretrained_model.s2t_model.load_state_dict(d)
pred = pretrained_model(sample_test_utterance['speech'])
print('PREDICTED: ' + pred[0][0])
print('REFERENCE: ' + sample_test_utterance['text_raw'])

"""### Inference with fine-tuned model"""

d = torch.load("./exp/finetune/1epoch.pth")
pretrained_model.s2t_model.load_state_dict(d)
pred = pretrained_model(sample_test_utterance['speech'])
print('PREDICTED: ' + pred[0][0])
print('REFERENCE: ' + sample_test_utterance['text_raw'])

"""## ✅ Task 1
Now that you have performed inference with both the pre-trained model and your fine-tuned model, provide some qualitative analyses of the results. How does the output between the two models differ? Do you observe any stylistic differences in the transcriptions?

(Your answer here)
"""



"""## ✅ Task 2

Following the inference sample, perform inference on the whole test dataset and report both WER and CER using jiwer.

## WER Calculation
"""

!pip install jiwer

"""### Inference with the original model"""

hyps = []
refs = []
d = torch.load("original.pth") # Default to ckpt before fine-tuning.
pretrained_model.s2t_model.load_state_dict(d)

# make sure we use GPU
pretrained_model.s2t_model.cuda()
pretrained_model.device = 'cuda'

## Start your implementation here

import jiwer


## Start your implementation here

# Compute WER and CER for the test datasets
wer_scores = ...
cer_scores = ...


print(f"Average WER: {wer_scores:.2%}")
print(f"Average CER: {cer_scores:.2%}")

"""### Inference with the fine-tuned model"""

hyps = []
refs = []
d = torch.load("./exp/finetune/1epoch.pth") # Default to ckpt before fine-tuning.
pretrained_model.s2t_model.load_state_dict(d)

# make sure we use GPU
pretrained_model.s2t_model.cuda()
pretrained_model.device = 'cuda'
## Start your implementation here

import jiwer



## Start your implementation here

# Compute WER and CER for the test datasets
wer_scores = ...
cer_scores = ...


print(f"Average WER: {wer_scores:.2%}")
print(f"Average CER: {cer_scores:.2%}")

