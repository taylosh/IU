If the n-grams were trained using essays by native speakers, how would you expect the probability of each sentence change, and why?

Because this model was trained on learners, it may be giving probability to 'incorrect' English, anything from mispellings to poorly constructed syntax. Also, it is likely that the non-native writers are using simpler constructions and vocabulary. If the model were to be trained on native writers, factors like these would add complexity to the expected language. This would reduce the probability of the non-natives' writing. That might be considered a benefit as it could indicate a level of 'correctness' in the writing, but it also might be too strict if it is meant to be used as a grading tool as we can't expect learners to write as complex as a native.